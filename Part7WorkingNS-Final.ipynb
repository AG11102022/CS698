{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f2ed71-0ec4-407f-9cee-a87e6a22b1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\algba\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "# --- Config ---\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "openai_client = OpenAI(api_key=\"")  # replace with yours\n",
    "qdrant = QdrantClient(host=\"localhost\", port=6333)\n",
    "collection_name = \"video_chunks\"\n",
    "\n",
    "CLIP_MAP = {\n",
    "    \"resnet\": [\"resnet_clip_3.mp4\", \"resnet_clip_4.mp4\"],\n",
    "    \"cnn_vs_fc\": [\"cnn_vs_fc_clip_1.mp4\", \"cnn_vs_fc_clip_2.mp4\"],\n",
    "    \"bce_loss\": [\"bce_loss_clip_1.mp4\", \"bce_loss_clip_2.mp4\"]\n",
    "}\n",
    "\n",
    "CLIP_PATH = \"retrieved_segments_filtered\"  # should be relative\n",
    "\n",
    "# --- Helper: Determine match category ---\n",
    "def get_predefined_key(question):\n",
    "    lowered = question.lower()\n",
    "    if \"resnet\" in lowered:\n",
    "        return \"resnet\"\n",
    "    elif \"cnn\" in lowered and \"fully connected\" in lowered:\n",
    "        return \"cnn_vs_fc\"\n",
    "    elif \"binary cross entropy\" in lowered:\n",
    "        return \"bce_loss\"\n",
    "    return None\n",
    "\n",
    "# --- Helper: Retrieve from Qdrant ---\n",
    "def retrieve_chunks(query, top_k=3):\n",
    "    vector = embedding_model.encode([query]).astype(\"float32\")[0]\n",
    "    results = qdrant.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=vector,\n",
    "        limit=top_k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    chunks = []\n",
    "    for hit in results:\n",
    "        payload = hit.payload\n",
    "        chunks.append({\n",
    "            \"text\": payload.get(\"text\", \"\"),\n",
    "            \"start\": payload.get(\"start\", 0),\n",
    "            \"video_id\": payload.get(\"video_id\", \"\"),\n",
    "            \"video_path\": payload.get(\"video_path\", \"\"),\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "# --- Helper: Convert seconds to hh:mm:ss ---\n",
    "def seconds_to_hms(seconds):\n",
    "    return str(timedelta(seconds=seconds))\n",
    "\n",
    "# --- Helper: Generate Answer ---\n",
    "def generate_answer(question, chunks):\n",
    "    context = \"\\n\\n\".join([c[\"text\"] for c in chunks])\n",
    "    system_prompt = f\"You are a helpful tutor. Use only the following context to answer the user's question.\\n\\nContext:\\n{context}\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# --- RAG Logic ---\n",
    "def rag_tutor(question, topic_filter=None):\n",
    "    try:\n",
    "        matched_key = get_predefined_key(question)\n",
    "\n",
    "        if matched_key:\n",
    "            local_clips = CLIP_MAP.get(matched_key, [])\n",
    "            if not local_clips:\n",
    "                return \"No local clips found for this topic.\", \"\", None\n",
    "                \n",
    "            chunks = retrieve_chunks(question, top_k=3)\n",
    "            answer = generate_answer(question, chunks)\n",
    "            links = \"\\n\\n\".join([f\"[Local Clip] {Path(clip).stem}\" for clip in local_clips])\n",
    "            return answer, links, os.path.join(CLIP_PATH, local_clips[0])\n",
    "\n",
    "        chunks = retrieve_chunks(question, top_k=3)\n",
    "        answer = generate_answer(question, chunks)\n",
    "        links = \"\\n\\n\".join([\n",
    "            f\"[{chunk['video_id']} - Chunk {i}] Start: {seconds_to_hms(chunk['start'])}\\nhttps://www.youtube.com/watch?v={chunk['video_id']}&t={chunk['start']}s\"\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ])\n",
    "        return answer, links, None\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", \"\", None\n",
    "\n",
    "# --- Gradio UI ---\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## VideoProj RAG Tutor\")\n",
    "    question_box = gr.Textbox(label=\"Ask a question\", placeholder=\"e.g., explain how ResNets work\")\n",
    "    topic_box = gr.Textbox(label=\"(Optional) Topic Filter (integer)\", placeholder=\"Leave blank to search all topics\")\n",
    "    submit_btn = gr.Button(\"Submit\")\n",
    "    answer_box = gr.Textbox(label=\"Answer\")\n",
    "    retrieved_box = gr.Textbox(label=\"Retrieved Chunks + Links\")\n",
    "    video_output = gr.Video(label=\"Relevant Clip (if local)\")\n",
    "\n",
    "    submit_btn.click(fn=rag_tutor, inputs=[question_box, topic_box], outputs=[answer_box, retrieved_box, video_output])\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
