{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e2568f8",
   "metadata": {},
   "source": [
    " **What trade-offs come with smaller vs. larger patch sizes in ViT?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290df67f",
   "metadata": {},
   "source": [
    "Smaller pactch sizes preserve more spatial details (e.g., edges), but this aspect leads to higher computational cost -increasing the number of tokens. With more tokens and parameters, the model may overfit, especially on small datasets.\n",
    "\n",
    "On the other hand, with larget patch sizes, and thus fewer tokens - computation cost is lower in attention layers. Fewer tokens may act as a form of regularization. Large patches, however, may skip over small but important visual patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f6153",
   "metadata": {},
   "source": [
    "**What inductive biases do CNNs have that ViTs lack? What are the consequences?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09f759",
   "metadata": {},
   "source": [
    "ViTs require large datasets and stronger regularization to learn spatial structure.\n",
    "\n",
    "CNNs naturally handle spatial shifts, requiring less data to learn position-invariant features. Also, they are more sample-efficient, especially for small datasets or fine grained patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f800683",
   "metadata": {},
   "source": [
    "**Why is positional encoding necessary in ViT, and how is it implemented?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acfed2a",
   "metadata": {},
   "source": [
    "Unlike CNNs, transformers have no inherent sense of order or position—they treat input tokens as a set, not a sequence or grid. It is typically implemented by adding learnable positional embeddings to each patch token, preserving the image's spatial structure. This allows the model to understand the relative positions of patches during attention-based processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057bf919",
   "metadata": {},
   "source": [
    "**What are the two separate encoders in CLIP, and what is their purpose?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ffdfa8",
   "metadata": {},
   "source": [
    "Image econder and text encoder.\n",
    "\n",
    "The goal is to project both image and text embeddings into a shared latent space, where semantically similar image-text pairs are close together. This enables tasks like zero-shot classification and image-text retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79fe1f",
   "metadata": {},
   "source": [
    "**Explain CLIP’s contrastive loss. How does it align image and text representations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc2ff9",
   "metadata": {},
   "source": [
    "CLIP uses a contrastive loss to align image and text embeddings in a shared space by encouraging matching pairs to have high similarity and non-matching pairs to be dissimilar. It computes cosine similarities between all image-text pairs in a batch and applies a symmetric InfoNCE loss. This training objective pulls related image-text pairs closer together while pushing unrelated pairs apart, enabling effective cross-modal understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff0264",
   "metadata": {},
   "source": [
    "**How does CLIP enable zero-shot classification? What role do prompts like “a photo of a ___” play?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb68fc8",
   "metadata": {},
   "source": [
    "CLIP enables zero-shot classification by comparing the similarity between an image and a set of text prompts representing each class label. Instead of training a classifier, it encodes the image and all class prompts (e.g., “a photo of a cat”, “a photo of a dog”) into a shared embedding space. These prompts guide the model to interpret the image in context, and the class whose text embedding is most similar to the image embedding is selected as the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab941ea",
   "metadata": {},
   "source": [
    "**What is the main difference in loss function between CLIP and SigLIP?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d6bf1",
   "metadata": {},
   "source": [
    "The main difference in the loss function between CLIP and SigLIP lies in how similarity is computed and optimized:\n",
    "\n",
    "CLIP:  softmax-based contrastive loss over the batch.\n",
    "SigLIP:  pairwise sigmoid-based binary loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4959f487",
   "metadata": {},
   "source": [
    "**What might be the impact of removing the softmax normalization across the batch in SigLIP?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb6137",
   "metadata": {},
   "source": [
    "Removing softmax makes training more flexible and stable, but may reduce the model’s ability to rank closely related samples comparatively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec1098",
   "metadata": {},
   "source": [
    "**What are potential advantages of SigLIP when deploying models in low-latency environments?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63042cef",
   "metadata": {},
   "source": [
    "SigLIP’s pairwise scoring and softmax-free design lead to faster, simpler, and more consistent inference.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
