{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a93d9de-a77d-43fa-8190-9f03973689a2",
   "metadata": {},
   "source": [
    "# This section:\r\n",
    "\r\n",
    "# *1. Uses FAISS to retrieve the most semantically relevant transcript chunks based on a user's question,*\r\n",
    "\r\n",
    "# *2. Constructs a context prompt from the retrieved chunks and sends it to OpenAI's GPT model via the chat API,*\r\n",
    "\r\n",
    "# *3. Generates a grounded answer using Retrieval-Augmented Generation (RAG), ensuring the response is based solely on the transcript content rather than general model knowledge.*\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecebe81-8f50-4405-b2fb-cda44524e2dd",
   "metadata": {},
   "source": [
    "### This section implements a Retrieval-Augmented Generation (RAG) pipeline — combining transcript chunk embeddings with OpenAI's GPT model.\n",
    "\n",
    "### When a user asks a question:\n",
    "\n",
    "### 1. It finds the most relevant transcript chunks using semantic search (via FAISS).\n",
    "\n",
    "### 2. It formats those chunks as context and sends them along with the question to GPT.\n",
    "\n",
    "### 3. GPT is instructed to only answer based on the provided text, ensuring that the answer is grounded in the actual video transcripts.\n",
    "\n",
    "### This approach enables accurate and context-aware answers tied directly to the processed transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01546cc-4911-41c4-824e-2d7b9e24c04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\algba\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Part 5: RAG + Topic-Aware Retrieval from Qdrant ---\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0204e0d-9fd2-4c66-a90e-ddab682d74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load embedding model and OpenAI client ---\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "client = OpenAI(api_key=\"\")  # Replace with your secure key or env var\n",
    "\n",
    "# --- Qdrant setup ---\n",
    "collection_name = \"video_chunks\"\n",
    "client_qdrant = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# --- Function: Search with optional topic filter ---\n",
    "def retrieve_chunks(question, topic_filter=None, top_k=3):\n",
    "    question_embedding = model.encode([question]).astype('float32')[0]\n",
    "\n",
    "    qdrant_filter = None\n",
    "    if topic_filter is not None:\n",
    "        qdrant_filter = Filter(\n",
    "            must=[FieldCondition(key=\"topic\", match=MatchValue(value=topic_filter))]\n",
    "        )\n",
    "\n",
    "    search_result = client_qdrant.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=question_embedding,\n",
    "        limit=top_k,\n",
    "        query_filter=qdrant_filter\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    for hit in search_result:\n",
    "        payload = hit.payload\n",
    "        chunks.append({\n",
    "            \"video_id\": payload[\"video_id\"],\n",
    "            \"chunk_id\": payload[\"chunk_id\"],\n",
    "            \"text\": payload[\"text\"],\n",
    "            \"start\": payload[\"start\"]\n",
    "        })\n",
    "    return pd.DataFrame(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc96749-f616-4881-aa3a-e94af16ff0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function: RAG answer using OpenAI ---\n",
    "def generate_answer(question, chunks_df):\n",
    "    context = \"\\n\\n\".join([f\"[{row.video_id} - chunk {row.chunk_id}]\\n{row.text}\" for _, row in chunks_df.iterrows()])\n",
    "    \n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. Use only the following context to answer the question.\\n\"\n",
    "        \"If unsure, say 'I don’t know.'\\n\\n\"\n",
    "        f\"Context:\\n{context}\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961f5ce3-43c5-492c-9f04-5c7ff4b398c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\algba\\AppData\\Local\\Temp\\ipykernel_24540\\2604927637.py:19: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = client_qdrant.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " Reinforcement learning involves an agent learning to make decisions by interacting with an environment and receiving rewards or penalties based on its actions. Exploration is a crucial aspect of reinforcement learning, as the agent needs to try out different actions to discover the best strategy for maximizing its rewards. By exploring different actions and observing the outcomes, the agent can learn which actions lead to positive rewards and adjust its decision-making accordingly.\n",
      "\n",
      "Retrieved Chunks:\n",
      "\n",
      "[Chunk 36 from t9zxmEHGT1s] — Start: 0:13:42\n",
      "Jump to: https://www.youtube.com/watch?v=t9zxmEHGT1s&t=822s\n",
      "from left to right uh so the whole agent is uh quite considerable uh uron Network so this is a uh a deep Network that accepts uh the uh row information uh or the sensing information the X under bar that we have seen earlier and uh directly spits out the uh steering Direction and the acceleration deceleration Alpha so now we have an end to endend uh system that uh is doing all the tasks that the previous subsystems the individual Subs subsystems were actually doing including perception probabilistic reion over time control and so on planning and so\n",
      "\n",
      "[Chunk 18 from 5gr4TTpVG5w] — Start: 0:06:51\n",
      "Jump to: https://www.youtube.com/watch?v=5gr4TTpVG5w&t=411s\n",
      "you're not really alone uh in the city so you are there are other agents other agents that are other cars or pedestrians whatever have you uh this prediction module is going to predict their intentions so given information that is coming to you from the perception sub system the prediction module is predicting where these actors are going next and based on that uh predictions the behavioral planner is effectively uh is implemented via what is called a final State machine and it is U planning for uh at a very fairly high level let's say negotiate an intersection\n",
      "\n",
      "[Chunk 38 from t9zxmEHGT1s] — Start: 0:14:28\n",
      "Jump to: https://www.youtube.com/watch?v=t9zxmEHGT1s&t=868s\n",
      "but in this kind of imitation learning what we actually have done is we have uh uh gotten kind of training data which are a bit uh different than the training data we were requiring from the earliest uh system uh here we have actually used uh humans in the loop uh to drive hum humans in the loop uh they drive the robot in the exactly the same environment that's kind of important and therefore we actually can pick up their own steering directions and accelerations decelerations uh and couple them with u so this is now my uh\n"
     ]
    }
   ],
   "source": [
    "# --- Example usage ---\n",
    "question = \"How does reinforcement learning relate to exploration?\"\n",
    "topic_number = 5  # (Optional) Set this to filter by a specific BERTopic topic\n",
    "\n",
    "retrieved_df = retrieve_chunks(question, topic_filter=topic_number, top_k=3)\n",
    "answer = generate_answer(question, retrieved_df)\n",
    "\n",
    "# --- Print results ---\n",
    "print(\"\\nAnswer:\\n\", answer)\n",
    "print(\"\\nRetrieved Chunks:\")\n",
    "for _, row in retrieved_df.iterrows():\n",
    "    start_time = int(row[\"start\"])\n",
    "    video_url = f\"https://www.youtube.com/watch?v={row.video_id}&t={start_time}s\"\n",
    "    print(f\"\\n[Chunk {row.chunk_id} from {row.video_id}] — Start: {timedelta(seconds=start_time)}\")\n",
    "    print(f\"Jump to: {video_url}\")\n",
    "    print(row.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
