{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c5380b-9202-4980-b7bf-b8f5fad8a4cf",
   "metadata": {},
   "source": [
    "# This section:\n",
    "\n",
    "# *1. Embeds each transcript chunk using sentence-transformers,*\n",
    "\n",
    "# *2. Builds a vector similarity index using FAISS for fast retrieval,*\n",
    "\n",
    "# *3. Saves both the vector index and metadata for use in querying or RAG-style applications.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5d620a-f257-4e56-8bf6-f9fbfdb56bf4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### This section takes the text chunks created earlier and transforms them into numerical embeddings that can be used to compare meaning between chunks ‚Äî for example, to find which part of a transcript best matches a user‚Äôs question.\n",
    "\n",
    "#### These embeddings are stored in a FAISS index, which is a high-speed search structure that allows you to quickly find the most similar chunks of text. This is the backbone for tasks like semantic search, where you want to retrieve the most relevant part of a video transcript given a natural language query.\n",
    "\n",
    "#### It also saves the metadata (like which chunk belongs to which video and when it starts) so that once a matching chunk is found, it can be tied back to its original video and timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb435b3-9250-4c22-b8a7-a48b8e062f27",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7fd9549-d424-4930-8752-e30839b87ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\algba\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Part 3: Chunk Embeddings + Upload to Qdrant ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae56b27-65d3-4f13-9840-49ac5884ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic on transcript chunks...\n",
      "Topics assigned: 692 unique topics\n",
      "Loading sentence transformer...\n",
      "üìê Encoding transcript chunks into vectors...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e3e992067e4b52be03929bfff44a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Qdrant and storing vectors...\n",
      "Collection 'video_chunks' already exists.\n",
      "Uploading to Qdrant in batches...\n",
      "Uploaded batch 1 of 36\n",
      "Uploaded batch 2 of 36\n",
      "Uploaded batch 3 of 36\n",
      "Uploaded batch 4 of 36\n",
      "Uploaded batch 5 of 36\n",
      "Uploaded batch 6 of 36\n",
      "Uploaded batch 7 of 36\n",
      "Uploaded batch 8 of 36\n",
      "Uploaded batch 9 of 36\n",
      "Uploaded batch 10 of 36\n",
      "Uploaded batch 11 of 36\n",
      "Uploaded batch 12 of 36\n",
      "Uploaded batch 13 of 36\n",
      "Uploaded batch 14 of 36\n",
      "Uploaded batch 15 of 36\n",
      "Uploaded batch 16 of 36\n",
      "Uploaded batch 17 of 36\n",
      "Uploaded batch 18 of 36\n",
      "Uploaded batch 19 of 36\n",
      "Uploaded batch 20 of 36\n",
      "Uploaded batch 21 of 36\n",
      "Uploaded batch 22 of 36\n",
      "Uploaded batch 23 of 36\n",
      "Uploaded batch 24 of 36\n",
      "Uploaded batch 25 of 36\n",
      "Uploaded batch 26 of 36\n",
      "Uploaded batch 27 of 36\n",
      "Uploaded batch 28 of 36\n",
      "Uploaded batch 29 of 36\n",
      "Uploaded batch 30 of 36\n",
      "Uploaded batch 31 of 36\n",
      "Uploaded batch 32 of 36\n",
      "Uploaded batch 33 of 36\n",
      "Uploaded batch 34 of 36\n",
      "Uploaded batch 35 of 36\n",
      "Uploaded batch 36 of 36\n"
     ]
    }
   ],
   "source": [
    "# --- Step 0: Load processed chunks ---\n",
    "chunk_df = pd.read_csv(\"VideoProj_chunks.csv\")  # Contains 'text', 'video_id', 'chunk_id', 'start'\n",
    "\n",
    "# --- Step 1: Apply BERTopic for topic modeling ---\n",
    "print(\"Running BERTopic on transcript chunks...\")\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(chunk_df[\"text\"])\n",
    "chunk_df[\"topic\"] = topics\n",
    "print(f\"Topics assigned: {len(set(topics))} unique topics\")\n",
    "\n",
    "# --- Step 2: Load sentence embedding model ---\n",
    "print(\"Loading sentence transformer...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# --- Step 3: Generate embeddings ---\n",
    "print(\"üìê Encoding transcript chunks into vectors...\")\n",
    "embeddings = model.encode(chunk_df[\"text\"].tolist(), show_progress_bar=True)\n",
    "embeddings = np.array(embeddings).astype('float32')  # Required format for Qdrant\n",
    "\n",
    "# --- Step 4: Connect to Qdrant ---\n",
    "print(\"Connecting to Qdrant and storing vectors...\")\n",
    "qdrant = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# --- Create collection if not already created ---\n",
    "if not qdrant.collection_exists(\"video_chunks\"):\n",
    "    qdrant.create_collection(\n",
    "        collection_name=\"video_chunks\",\n",
    "        vectors_config=VectorParams(size=embeddings.shape[1], distance=Distance.COSINE)\n",
    "    )\n",
    "else:\n",
    "    print(\"Collection 'video_chunks' already exists.\")\n",
    "\n",
    "# --- Sanitize payload to be JSON-safe ---\n",
    "chunk_df = chunk_df.fillna(\"\")  # Remove NaNs\n",
    "\n",
    "def sanitize_payload(row):\n",
    "    return {\n",
    "        \"video_id\": str(row[\"video_id\"]),\n",
    "        \"chunk_id\": int(row[\"chunk_id\"]),\n",
    "        \"text\": str(row[\"text\"]),\n",
    "        \"start\": int(row[\"start\"]),\n",
    "        \"topic\": int(row[\"topic\"])\n",
    "    }\n",
    "\n",
    "# --- Build all points ---\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=str(uuid.uuid4()),\n",
    "        vector=vec.tolist(),\n",
    "        payload=sanitize_payload(row)\n",
    "    )\n",
    "    for vec, (_, row) in zip(embeddings, chunk_df.iterrows())\n",
    "]\n",
    "\n",
    "# --- Upload in batches to avoid size limits ---\n",
    "print(\"Uploading to Qdrant in batches...\")\n",
    "batch_size = 1000\n",
    "for i in range(0, len(points), batch_size):\n",
    "    batch = points[i:i + batch_size]\n",
    "    qdrant.upsert(collection_name=\"video_chunks\", points=batch)\n",
    "    print(f\"Uploaded batch {i // batch_size + 1} of {len(points) // batch_size + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e50e514-2140-4289-bdf0-657e76345719",
   "metadata": {},
   "source": [
    "### Embed Chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c46c63-35fc-49e9-a3e1-18c69be6852f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to: VideoProj_metadata.pkl\n",
      "\n",
      "Preview of stored data:\n",
      "      video_id  chunk_id  start  topic\n",
      "0  -Hv6OPTlUZU         0      0     40\n",
      "1  -Hv6OPTlUZU         1      2     83\n",
      "2  -Hv6OPTlUZU         2      4     18\n",
      "3  -Hv6OPTlUZU         3      5    621\n",
      "4  -Hv6OPTlUZU         4      7     16\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Save metadata ---\n",
    "with open(\"VideoProj_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunk_df, f)\n",
    "print(\"Metadata saved to: VideoProj_metadata.pkl\")\n",
    "\n",
    "# --- Step 6: Preview ---\n",
    "print(\"\\nPreview of stored data:\")\n",
    "print(chunk_df[['video_id', 'chunk_id', 'start', 'topic']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
